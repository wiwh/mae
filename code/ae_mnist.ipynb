{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST auto-encoder\n",
    "Here we build an auto-encoder for the MNIST dataset. Later, we will use the same network but make it generative with the M-estimator Auto-encoder (MAE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=\"true\",\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=\"test\",\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(training_data, batch_size=64, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            # 1 x 28 x 28\n",
    "            nn.Conv2d(1, 4, kernel_size=5),\n",
    "            # 4 x 24 x 24 = 2304\n",
    "            nn.Flatten(),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(2304, 10),\n",
    "            # 10\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(10, 2)\n",
    "            # 2\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            # 2\n",
    "            nn.Linear(2, 10),\n",
    "            # 10\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(10, 2304),\n",
    "            # 2304\n",
    "            nn.ReLU(True),\n",
    "            nn.Unflatten(1, (4, 24, 24)),\n",
    "            # 4 x 24 x 24\n",
    "            nn.ConvTranspose2d(4, 1, kernel_size = 5),\n",
    "            # 1 x 28 x 28\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "model = AutoEncoder()\n",
    "distance = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "# fit\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for data in dataloader:\n",
    "        img, _ = data\n",
    "        output = model(img)\n",
    "        loss = distance(output, img)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('epoch [{}/{}], loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The simplest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/100], loss: 1.2964\n",
      "epoch [2/100], loss: 1.2807\n",
      "epoch [3/100], loss: 1.2785\n",
      "epoch [4/100], loss: 1.3064\n",
      "epoch [5/100], loss: 1.2748\n",
      "epoch [6/100], loss: 1.2777\n",
      "epoch [7/100], loss: 1.2894\n",
      "epoch [8/100], loss: 1.2684\n",
      "epoch [9/100], loss: 1.3062\n",
      "epoch [10/100], loss: 1.2859\n",
      "epoch [11/100], loss: 1.3150\n",
      "epoch [12/100], loss: 1.2937\n",
      "epoch [13/100], loss: 1.2624\n",
      "epoch [14/100], loss: 1.2819\n",
      "epoch [15/100], loss: 1.2332\n",
      "epoch [16/100], loss: 1.2860\n",
      "epoch [17/100], loss: 1.2503\n",
      "epoch [18/100], loss: 1.2526\n",
      "epoch [19/100], loss: 1.2413\n",
      "epoch [20/100], loss: 1.3144\n",
      "epoch [21/100], loss: 1.2939\n",
      "epoch [22/100], loss: 1.2739\n",
      "epoch [23/100], loss: 1.2522\n",
      "epoch [24/100], loss: 1.2748\n",
      "epoch [25/100], loss: 1.2599\n",
      "epoch [26/100], loss: 1.2202\n",
      "epoch [27/100], loss: 1.2535\n",
      "epoch [28/100], loss: 1.2501\n",
      "epoch [29/100], loss: 1.2921\n",
      "epoch [30/100], loss: 1.2614\n",
      "epoch [31/100], loss: 1.2537\n",
      "epoch [32/100], loss: 1.2547\n",
      "epoch [33/100], loss: 1.2518\n",
      "epoch [34/100], loss: 1.2153\n",
      "epoch [35/100], loss: 1.2396\n",
      "epoch [36/100], loss: 1.2167\n",
      "epoch [37/100], loss: 1.2487\n",
      "epoch [38/100], loss: 1.2028\n",
      "epoch [39/100], loss: 1.2976\n",
      "epoch [40/100], loss: 1.2300\n",
      "epoch [41/100], loss: 1.2329\n",
      "epoch [42/100], loss: 1.2412\n",
      "epoch [43/100], loss: 1.2127\n",
      "epoch [44/100], loss: 1.1945\n",
      "epoch [45/100], loss: 1.2858\n",
      "epoch [46/100], loss: 1.2602\n",
      "epoch [47/100], loss: 1.2157\n",
      "epoch [48/100], loss: 1.2381\n",
      "epoch [49/100], loss: 1.2482\n",
      "epoch [50/100], loss: 1.2590\n",
      "epoch [51/100], loss: 1.3227\n",
      "epoch [52/100], loss: 1.2090\n",
      "epoch [53/100], loss: 1.2088\n",
      "epoch [54/100], loss: 1.1995\n",
      "epoch [55/100], loss: 1.2423\n",
      "epoch [56/100], loss: 1.2561\n",
      "epoch [57/100], loss: 1.2230\n",
      "epoch [58/100], loss: 1.2013\n",
      "epoch [59/100], loss: 1.2144\n",
      "epoch [60/100], loss: 1.2312\n",
      "epoch [61/100], loss: 1.2247\n",
      "epoch [62/100], loss: 1.2564\n",
      "epoch [63/100], loss: 1.2650\n",
      "epoch [64/100], loss: 1.2014\n",
      "epoch [65/100], loss: 1.2610\n",
      "epoch [66/100], loss: 1.2755\n",
      "epoch [67/100], loss: 1.1766\n",
      "epoch [68/100], loss: 1.2514\n",
      "epoch [69/100], loss: 1.2641\n",
      "epoch [70/100], loss: 1.1991\n",
      "epoch [71/100], loss: 1.2429\n",
      "epoch [72/100], loss: 1.2492\n",
      "epoch [73/100], loss: 1.2502\n",
      "epoch [74/100], loss: 1.2360\n",
      "epoch [75/100], loss: 1.2400\n",
      "epoch [76/100], loss: 1.2216\n",
      "epoch [77/100], loss: 1.2280\n",
      "epoch [78/100], loss: 1.2631\n",
      "epoch [79/100], loss: 1.2404\n",
      "epoch [80/100], loss: 1.2138\n",
      "epoch [81/100], loss: 1.2573\n",
      "epoch [82/100], loss: 1.2417\n",
      "epoch [83/100], loss: 1.2242\n",
      "epoch [84/100], loss: 1.2891\n",
      "epoch [85/100], loss: 1.2252\n",
      "epoch [86/100], loss: 1.2634\n",
      "epoch [87/100], loss: 1.2584\n",
      "epoch [88/100], loss: 1.2474\n",
      "epoch [89/100], loss: 1.2472\n",
      "epoch [90/100], loss: 1.1548\n",
      "epoch [91/100], loss: 1.2213\n",
      "epoch [92/100], loss: 1.2349\n",
      "epoch [93/100], loss: 1.2149\n",
      "epoch [94/100], loss: 1.2616\n",
      "epoch [95/100], loss: 1.2926\n",
      "epoch [96/100], loss: 1.2556\n",
      "epoch [97/100], loss: 1.2141\n",
      "epoch [98/100], loss: 1.2571\n",
      "epoch [99/100], loss: 1.1729\n",
      "epoch [100/100], loss: 1.2221\n"
     ]
    }
   ],
   "source": [
    "class AutoEncoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            # 1 x 28 x 28 = 784\n",
    "            nn.Flatten(),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(784, 2)\n",
    "            # 2\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2, 784),\n",
    "            # 784\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (1, 28, 28)),\n",
    "            # 1 x 28 x 28\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "model = AutoEncoder()\n",
    "distance = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "# fit\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for data in dataloader:\n",
    "        img, _ = data\n",
    "        output = model(img)\n",
    "        loss = distance(output, img)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('epoch [{}/{}], loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "02379b91627ead677140c441995c323138285dea806245dca7a173ada35ac023"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
