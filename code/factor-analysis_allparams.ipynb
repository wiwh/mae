{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating Factor Analysis\n",
    "One of the simplest model for generative auto-encoders is Factor Analysis. We'll use our method to learn it, with the loss function\n",
    "\n",
    "\n",
    "$$ \\mathcal L(\\boldsymbol \\theta, y^{(i)})  = {y^{(i)}}^\\top \\hat y_{\\boldsymbol \\theta}^{(i)} - \\mathbb E_{p_{\\boldsymbol\\theta}}[y^\\top \\hat y_{\\boldsymbol \\theta}]$$\n",
    "\n",
    "where $\\hat y_{\\boldsymbol \\theta} = \\text{decoder}_{\\boldsymbol \\theta}(\\text{encoder}_{\\boldsymbol \\theta}(y))$ and $y\\sim p_{\\boldsymbol \\theta}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIMENSIONS\n",
    "n = 100\n",
    "p = 50\n",
    "q = 5\n",
    "DIMENSION_Y = (n, p)\n",
    "DIMENSION_Z = (n, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Initialize Module\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        self.var_y = torch.ones(p)\n",
    "\n",
    "    def forward(self, y):\n",
    "        z = self.encoder(y)\n",
    "        y = self.decoder(z)\n",
    "        return y\n",
    "    \n",
    "    def sample(self, z = None):\n",
    "        \"\"\"Sample from the fitted model.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            if z is None:\n",
    "                z = torch.randn(DIMENSION_Z)\n",
    "\n",
    "            eps = torch.randn(DIMENSION_Y) * torch.sqrt(self.var_y)\n",
    "            y = self.decoder(z) + eps\n",
    "        \n",
    "        return (y, z)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define parameters\n",
    "        self.w = nn.Parameter(torch.randn(q, p))\n",
    "    \n",
    "    # override forward\n",
    "    def forward(self, z):\n",
    "        linpar = z @ self.w\n",
    "        return linpar\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define a sequential model\n",
    "        self.encoder_model = nn.Sequential(\n",
    "            nn.Linear(in_features = p, out_features = 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features = 100, out_features = 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=100, out_features = q)\n",
    "        )\n",
    "\n",
    "    def forward(self, y):\n",
    "        return self.encoder_model(y)\n",
    "\n",
    "class GMLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input, target, input_sim, target_sim, z_hat, z_sim):\n",
    "        loss = -torch.mean(input * target - input_sim * target_sim) + torch.mean(torch.pow(z_hat - z_sim, 2))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Loss 6.32\n",
      "Epoch 20 Loss 6.23\n",
      "Epoch 30 Loss 6.15\n",
      "Epoch 40 Loss 6.07\n",
      "Epoch 50 Loss 6.02\n",
      "Epoch 60 Loss 5.98\n",
      "Epoch 70 Loss 5.94\n",
      "Epoch 80 Loss 5.91\n",
      "Epoch 90 Loss 5.90\n",
      "Epoch 100 Loss 5.90\n",
      "Epoch 110 Loss 5.90\n",
      "Epoch 120 Loss 5.91\n",
      "Epoch 130 Loss 5.98\n",
      "Epoch 140 Loss 6.09\n",
      "Epoch 150 Loss 6.30\n",
      "Epoch 160 Loss 6.63\n",
      "Epoch 170 Loss 7.13\n",
      "Epoch 180 Loss 7.86\n",
      "Epoch 190 Loss 8.88\n",
      "Epoch 200 Loss 10.25\n",
      "Epoch 210 Loss 12.03\n",
      "Epoch 220 Loss 14.34\n",
      "Epoch 230 Loss 17.25\n",
      "Epoch 240 Loss 20.88\n",
      "Epoch 250 Loss 25.35\n",
      "Epoch 260 Loss 30.88\n",
      "Epoch 270 Loss 37.91\n",
      "Epoch 280 Loss 46.44\n",
      "Epoch 290 Loss 56.57\n",
      "Epoch 300 Loss 69.13\n",
      "Epoch 310 Loss 84.20\n",
      "Epoch 320 Loss 102.34\n",
      "Epoch 330 Loss 125.58\n",
      "Epoch 340 Loss 153.76\n",
      "Epoch 350 Loss 186.35\n",
      "Epoch 360 Loss 227.56\n",
      "Epoch 370 Loss 275.68\n",
      "Epoch 380 Loss 335.69\n",
      "Epoch 390 Loss 407.63\n",
      "Epoch 400 Loss 493.31\n",
      "Epoch 410 Loss 592.38\n",
      "Epoch 420 Loss 717.10\n",
      "Epoch 430 Loss 860.74\n",
      "Epoch 440 Loss 1016.44\n",
      "Epoch 450 Loss 1200.59\n",
      "Epoch 460 Loss 1418.50\n",
      "Epoch 470 Loss 1664.98\n",
      "Epoch 480 Loss 1947.45\n",
      "Epoch 490 Loss 2248.11\n",
      "Epoch 500 Loss 2595.17\n",
      "Epoch 510 Loss 3000.39\n",
      "Epoch 520 Loss 3445.11\n",
      "Epoch 530 Loss 3919.85\n",
      "Epoch 540 Loss 4396.86\n",
      "Epoch 550 Loss 5013.22\n",
      "Epoch 560 Loss 5729.06\n",
      "Epoch 570 Loss 6352.59\n",
      "Epoch 580 Loss 7084.87\n",
      "Epoch 590 Loss 7836.87\n",
      "Epoch 600 Loss 8793.50\n",
      "Epoch 610 Loss 9953.78\n",
      "Epoch 620 Loss 10979.58\n",
      "Epoch 630 Loss 12090.63\n",
      "Epoch 640 Loss 13167.38\n",
      "Epoch 650 Loss 14121.38\n",
      "Epoch 660 Loss 15691.02\n",
      "Epoch 670 Loss 17329.06\n",
      "Epoch 680 Loss 18912.75\n",
      "Epoch 690 Loss 20269.01\n",
      "Epoch 700 Loss 21885.53\n",
      "Epoch 710 Loss 23970.06\n",
      "Epoch 720 Loss 25895.83\n",
      "Epoch 730 Loss 27901.65\n",
      "Epoch 740 Loss 30548.00\n",
      "Epoch 750 Loss 32930.05\n",
      "Epoch 760 Loss 35453.41\n",
      "Epoch 770 Loss 38542.21\n",
      "Epoch 780 Loss 41396.05\n",
      "Epoch 790 Loss 44445.46\n",
      "Epoch 800 Loss 48265.02\n",
      "Epoch 810 Loss 52526.63\n",
      "Epoch 820 Loss 57306.78\n",
      "Epoch 830 Loss 61272.41\n",
      "Epoch 840 Loss 64557.98\n",
      "Epoch 850 Loss 69172.62\n",
      "Epoch 860 Loss 74457.23\n",
      "Epoch 870 Loss 79272.51\n",
      "Epoch 880 Loss 85035.35\n",
      "Epoch 890 Loss 90090.35\n",
      "Epoch 900 Loss 96719.20\n",
      "Epoch 910 Loss 103098.38\n",
      "Epoch 920 Loss 109536.95\n",
      "Epoch 930 Loss 113438.98\n",
      "Epoch 940 Loss 121214.96\n",
      "Epoch 950 Loss 127605.71\n",
      "Epoch 960 Loss 135407.88\n",
      "Epoch 970 Loss 143444.36\n",
      "Epoch 980 Loss 153976.14\n",
      "Epoch 990 Loss 162574.95\n",
      "Epoch 1000 Loss 172900.05\n"
     ]
    }
   ],
   "source": [
    "model_true = FA()\n",
    "y_true, z_true = model_true.sample()\n",
    "assert y_true.shape == DIMENSION_Y and z_true.shape == DIMENSION_Z\n",
    "model = FA()\n",
    "loss_fn = GMLoss()\n",
    "# Fit the model\n",
    "epochs = 1000\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Fit eval: MSE\n",
    "eval_fn = nn.MSELoss()\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    target_true = model(y_true)\n",
    "\n",
    "    # Simulate and forward pass on the simulated sample\n",
    "    y_sim, z_sim = model.sample()\n",
    "    z_hat = model.encoder(y_sim)\n",
    "    target_sim = model.decoder(z_hat)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = loss_fn(y_true, target_true, y_sim, target_sim, z_hat, z_sim)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Eval the fit every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "            eval = eval_fn(y_true, target_true)\n",
    "        print(\"Epoch {} Loss {:.2f}\".format(epoch, eval))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
